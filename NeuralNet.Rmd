---
title: "Prediction using neural network"
author: "Prem Kumar Kamasani"
date: "January 13, 2019"
output: html_document

```{r}
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
data <- read.csv(file="C:/Users/PREM/Desktop/Advance Machine Learning/Project/BlackFriday.csv", header=TRUE, sep=",")
str(data)
```
Since we cannot apply neural network model on factor variables we convert all factor variables to numeric.
```{r}
data %<>% mutate_if(is.factor,as.numeric)
str(data)
```
Handling Missing Data

```{r}
colnames(data)[ apply(data, 2, anyNA) ]
data[is.na(data)] <- 0
```
Only "Product_Category_2", "Product_Category_3" columns have missing values. So we replace NA with 0.

```{r}
data[is.na(data)] <- 0
str(data)
#neural Net visualization
 
n<-neuralnet(Purchase~ User_ID+Product_ID+Gender+Age+Occupation+City_Category+Stay_In_Current_City_Years+Marital_Status+Product_Category_1+Product_Category_2+Product_Category_3,data = data, hidden=c(10,5), linear.output = F, lifesign = 'full', rep = 1)
plot(n,col.hidden='darkred',col.hidden.synapse = 'blue',fill='lightgreen',information=F,show.weights = F)

```
splitting data into train and test data with 75% and 25% respectively 
```{r}
data<-as.matrix(data)
dimnames(data)<-NULL
set.seed(42)
ind<-sample(2,nrow(data), replace=T, prob=c(0.75,0.25))
x_train <-data[ind == 1, 1:11]
x_test <- data[ind == 2, 1:11]
y_train <- data[ind == 1 , 12]
y_test <- data[ind == 2, 12]
```
```{r}
# normalization to train data.
m<-colMeans(x_train)
s<-apply(x_train,2,sd)
x_train<-scale(x_train,center=m,scale=s)
x_test<-scale(x_test, center = m, scale=s)
```
Model is created using all the predictor variables.
```{r}
# create model
model<- keras_model_sequential()
model%>% layer_dense(units=100, activation = 'relu', input_shape = c(11)) %>% 
  layer_dropout(0.4)%>% 
  layer_dense(units=50, activation = 'relu') %>%
  layer_dropout(0.3)%>% 
  layer_dense(units=20, activation = 'relu') %>%
  layer_dropout(0.2)%>% 
  layer_dense(units=1)
model%>% compile(loss='mse', optimizer='rmsprop', metrics="mae")
mymodel<- model %>% fit(x_train,y_train,epochs=100,batch_size=32,validation_split=0.2)
model%>%evaluate(x_test,y_test)
pred<-model%>%predict(x_test)
```
Now we consider only low level (age, gender, etc.,) predictor variables to train the data.

```{r}
set.seed(42)
ind<-sample(2,nrow(data), replace=T, prob=c(0.75,0.25))
x_train <-data[ind == 1, 3:11]
x_test <- data[ind == 2, 3:11]
y_train <- data[ind == 1 , 12]
y_test <- data[ind == 2, 12]
# normalize
m<-colMeans(x_train)
s<-apply(x_train,2,sd)
x_train<-scale(x_train,center=m,scale=s)
x_test<-scale(x_test, center = m, scale=s)
# create model
model<- keras_model_sequential()
model%>% layer_dense(units=100, activation = 'relu', input_shape = c(9)) %>% 
  layer_dropout(0.4)%>% 
  layer_dense(units=50, activation = 'relu') %>%
  layer_dropout(0.3)%>% 
  layer_dense(units=20, activation = 'relu') %>%
  layer_dropout(0.2)%>% 
  layer_dense(units=1)
model%>% compile(loss='mse', optimizer='rmsprop', metrics="mae")
mymodel<- model %>% fit(x_train,y_train,epochs=100,batch_size=32,validation_split=0.2)
model%>%evaluate(x_test,y_test)
pred<-model%>%predict(x_test)
summary(model)
```
